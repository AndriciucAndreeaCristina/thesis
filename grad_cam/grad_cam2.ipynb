{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_folder = \"Sharks/\"\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define a 1 image dataset\n",
    "root = \"./data/\" + class_folder\n",
    "dataset = datasets.ImageFolder(root=root, transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_gpu\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\env_gpu\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # get the pretrained VGG19 network\n",
    "        self.vgg = vgg19(pretrained=True)\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.vgg.features[:36]\n",
    "        \n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view((x.size(0), -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation extraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the VGG model\n",
    "vgg = VGG()\n",
    "\n",
    "# set the evaluation mode\n",
    "vgg.eval()\n",
    "predictions = []\n",
    "\n",
    "# get the image from the dataloader\n",
    "for img, label in dataloader:\n",
    "    # img, _ = next(iter(dataloader))\n",
    "    # print(img.shape)\n",
    "    # get the most likely prediction of the model\n",
    "    pred = vgg(img)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photo index  0\n",
      "    Predicted: (great white shark, 97.29%)\n",
      "    Predicted: (tiger shark, 2.28%)\n",
      "    Predicted: (hammerhead shark, 0.42%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Load ImageNet class labels\n",
    "LABELS_URL = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
    "response = requests.get(LABELS_URL)\n",
    "labels = response.json()\n",
    "\n",
    "for (idx, pred) in enumerate(predictions):\n",
    "    \n",
    "    _, indices = torch.topk(pred, 3)\n",
    "    probabilities = torch.nn.functional.softmax(pred, dim=1)[0] * 100\n",
    "    print(\"Photo index \", idx)\n",
    "    # Print the predictions\n",
    "    for idx in indices[0]:\n",
    "        label = labels[idx]\n",
    "        prob = probabilities[idx].item()\n",
    "        print(f\"    Predicted: ({label}, {prob:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dimensions = [ int(pred.argmax(dim=1)[0]) for pred in predictions]\n",
    "\n",
    "# for (idx, dimension) in enumerate(dimensions):\n",
    "#     # get the gradient of the output with respect to the parameters of the model\n",
    "#     pred[:, dimension].backward()\n",
    "\n",
    "#     # pull the gradients out of the model\n",
    "#     gradients = vgg.get_activations_gradient()\n",
    "\n",
    "#     # pool the gradients across the channels\n",
    "#     pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "#     # get the activations of the last convolutional layer\n",
    "#     activations = vgg.get_activations(img).detach()\n",
    "\n",
    "#     # weight the channels by corresponding gradients\n",
    "#     for i in range(512):\n",
    "#         activations[:, i, :, :] *= pooled_gradients[i]\n",
    "        \n",
    "#     # average the channels of the activations\n",
    "#     heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "#     # relu on top of the heatmap\n",
    "#     # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "#     heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "#     # normalize the heatmap\n",
    "#     heatmap /= torch.max(heatmap)\n",
    "\n",
    "#     # draw the heatmap\n",
    "#     plt.matshow(heatmap.squeeze())\n",
    "#     path_save = \"heatmap_raw\" + str(idx) + \".pdf\"\n",
    "#     plt.savefig(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(model, img): \n",
    "    pred = model(img)\n",
    "    dimension = int(pred.argmax(dim=1)[0])\n",
    "    pred[:, dimension].backward()\n",
    "    gradients = model.get_activations_gradient()\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    activations = model.get_activations(img).detach()\n",
    "    for i in range(activations.shape[1]):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap.cpu().numpy(), 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_heatmap(heatmap, idx, output_folder):\n",
    "    plt.matshow(heatmap.squeeze())\n",
    "    path_save = os.path.join(output_folder, f\"heatmap_raw_{idx}.pdf\")\n",
    "    plt.savefig(path_save)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superimpose_and_save(heatmap, img_path, idx, output_folder):\n",
    "    img = cv2.imread(img_path)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + img\n",
    "    superimposed_path = os.path.join(output_folder, f\"superimposed_{idx}.jpg\")\n",
    "    cv2.imwrite(superimposed_path, superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Superimposed images saved to: ./data/Elephants/output_images/\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "\n",
    "# img = cv2.imread('./data/Elephants/data/02.jpg')\n",
    "\n",
    "# heatmap = np.array(heatmap)\n",
    "\n",
    "# heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "# heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# superimposed_img = heatmap * 0.4 + img\n",
    "\n",
    "# cv2.imwrite('./map.jpg', superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./data/\" + class_folder + \"outputs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for idx, (img, _) in enumerate(dataloader):\n",
    "    heatmap = generate_heatmap(vgg, img)\n",
    "    save_heatmap(heatmap, idx, output_folder)\n",
    "    img_path = dataloader.dataset.imgs[idx][0]\n",
    "    superimpose_and_save(heatmap, img_path, idx, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
